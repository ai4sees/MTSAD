{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MOUNTING AND NAVIGATING TO THE DIRECTORY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  MOUNTING THE GOOGLE DRIVE TO GOOGLE COLAB NOTEBOOK\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# NAVIGATE TO THE FOLDER\n",
    "# %cd /content/drive/MyDrive/updated_code/final_code_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSTALLATION OF THE REQUIREMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  RUN THE FOLLOWING COMMAND FOR INSTALLING THE REQUIREMENTS\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note 1:  Restart the runtime for the installed packages to take effect by commenting the above install command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note 2: Now go to the following path /usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py and at line 387 remove the argument “is_causal = is_causal”\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should look like this output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESS THE DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSL test_label (73729,)\n",
      "MSL train (58317, 55)\n",
      "MSL test (73729, 55)\n",
      "^C\n",
      "SMAP test_label (427617,)\n",
      "SMAP train (135183, 25)\n",
      "SMAP test (427617, 25)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python preprocess.py --dataset MSL\n",
    "!python preprocess.py --dataset SMAP\n",
    "!python preprocess.py --dataset SMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUN THE CODE SPECIFYING THE DATASET AND ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data of: MSL\n",
      "train:  0 None\n",
      "test:  0 None\n",
      "Data normalized\n",
      "Data normalized\n",
      "train set shape:  (58317, 55)\n",
      "test set shape:  (73729, 55)\n",
      "test set label shape:  (73729,)\n",
      "Will forecast and reconstruct input features: [0]\n",
      "train_size: 50688\n",
      "validation_size: 5632\n",
      "test_size: 73472\n",
      "TRAN_ENC(\n",
      "  (conv): ConvLayer(\n",
      "    (padding): ConstantPad1d(padding=(3, 3), value=0.0)\n",
      "    (conv): Conv1d(55, 55, kernel_size=(7,), stride=(1,))\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (feature_encoders): ModuleList(\n",
      "    (0): Transformer_encoder(\n",
      "      (learn_pos_enc): LearnablePositionalEmbedding()\n",
      "      (pos_encoder): PositionalEncoding(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (transformer_encoder): TransformerEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=55, out_features=55, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=55, out_features=16, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=16, out_features=55, bias=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (layer_norm): LayerNorm((55,), eps=1e-05, elementwise_affine=True)\n",
      "            (activation): LeakyReLU(negative_slope=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (time_encoders): ModuleList(\n",
      "    (0): Transformer_encoder(\n",
      "      (learn_pos_enc): LearnablePositionalEmbedding()\n",
      "      (pos_encoder): PositionalEncoding(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (transformer_encoder): TransformerEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=30, out_features=30, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=30, out_features=16, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=16, out_features=30, bias=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (layer_norm): LayerNorm((30,), eps=1e-05, elementwise_affine=True)\n",
      "            (activation): LeakyReLU(negative_slope=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_encoders): ModuleList(\n",
      "    (0): Transformer_encoder(\n",
      "      (learn_pos_enc): LearnablePositionalEmbedding()\n",
      "      (pos_encoder): PositionalEncoding(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (transformer_encoder): TransformerEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=165, out_features=165, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=165, out_features=16, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=16, out_features=165, bias=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (layer_norm): LayerNorm((165,), eps=1e-05, elementwise_affine=True)\n",
      "            (activation): LeakyReLU(negative_slope=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv1d): Conv1dLayer(\n",
      "    (padding1d): ConstantPad1d(padding=(0, 0), value=0.0)\n",
      "    (conv1d): Conv1d(165, 165, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (gru): GRULayer(\n",
      "    (gru): GRU(165, 150, num_layers=5, batch_first=True, dropout=0.3)\n",
      "  )\n",
      "  (forecasting_model): Forecasting_Model(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x Linear(in_features=150, out_features=150, bias=True)\n",
      "      (3): Linear(in_features=150, out_features=1, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (recon_model): ReconstructionModel(\n",
      "    (decoder): RNNDecoder(\n",
      "      (rnn): GRU(150, 150, batch_first=True)\n",
      "    )\n",
      "    (fc): Linear(in_features=150, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Saathvika\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataset MSL --lookback 30 --epochs 5 --use_gatv2 False --arch 1-1-1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24438,
     "status": "ok",
     "timestamp": 1703040269831,
     "user": {
      "displayName": "Mehaboob Basha",
      "userId": "06121720411973053130"
     },
     "user_tz": -330
    },
    "id": "4UeV4x60cAOI",
    "outputId": "ff1ae279-d3bf-481b-aae2-27b67d4ece28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# connect the colab notebook to your Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 365,
     "status": "ok",
     "timestamp": 1703040288616,
     "user": {
      "displayName": "Mehaboob Basha",
      "userId": "06121720411973053130"
     },
     "user_tz": -330
    },
    "id": "P0ScZwvQE62Y",
    "outputId": "778425c4-19a3-4ef4-cbe8-99897a9e6032"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/EXTRA/Kartik/final_code_file\n"
     ]
    }
   ],
   "source": [
    "# Go inside the folder where all the code files are kept using the following command\n",
    "\n",
    "%cd /content/drive/MyDrive/EXTRA/Kartik/final_code_file\n",
    "# %cd /content/drive/MyDrive/EXTRA/Kartik/final_code_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BD4axkvVF1tp"
   },
   "outputs": [],
   "source": [
    "# Now install all the necessary packages by running the next command\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PCMde5nmF9PW"
   },
   "outputs": [],
   "source": [
    "# after running this, comment it and restart the runtime for the installed packages to take effect.\n",
    "# You only need to run it once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5UWPF0OWylt"
   },
   "outputs": [],
   "source": [
    "# Now go to the following path /usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py and at line 387 remove the argument “is_causal = is_causal”\n",
    "# It should look like this output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRYtigDIGXFS"
   },
   "outputs": [],
   "source": [
    "# Below is the final command to start trainng the model\n",
    "# Before that ensure the folllowing things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHGwkNFRS-4f"
   },
   "source": [
    "\n",
    "1. ARCHITECTURE:\n",
    "  1. There are two architectures available. One with one-one transformer each and second with 4-4 transformers each for feature-temporal domain.\n",
    "  There are two python files which contain their code. Copy the code of that architecture which you have selected and paste it into the \"mtad_gat.py\" file\n",
    "\n",
    "2. DATASET:\n",
    "  1. if you have to train on SMAP dataset then\n",
    "    1. change the arg name in the command line to SMAP for ex. (`--dataset SMAP`)\n",
    "    2. Open mtad_gat.py file and in the line 66-69 change the 2nd argument to 5\n",
    "    \n",
    "    the line should look like this \"`Transformer_encoder(n_features, 5, sequence_length=window_size, output_dim=n_features)`\"\n",
    "\n",
    "  1. Similarly for MSL:\n",
    "    1. change (`--dataset MSL`)\n",
    "    2. Open mtad_gat.py file, check and change line 66-69\n",
    "    For Ex. \" `Transformer_encoder(n_features, 11, sequence_length=window_size, output_dim=n_features)` \"\n",
    "\n",
    "  1. Similarly for SMD:\n",
    "    1. change (`--dataset SMD`)\n",
    "    2. Open mtad_gat.py file, check and change line 66-69\n",
    "    For Ex. \" `Transformer_encoder(n_features, 19, sequence_length=window_size, output_dim=n_features)` \"\n",
    "\n",
    "3. Window Size:\n",
    "  General Instructions for window size - \"x\"\n",
    "  1. change argument in command line (`--lookback x`)\n",
    "  2. Open train.py, change line 45-49 to this\n",
    "    \n",
    "\n",
    "```\n",
    "x_t = (x_train.shape[0] - x)//2560\n",
    "y_t = (x_test.shape[0]//256 - 1) * 256\n",
    "x_train = x_train[:x_t*2560 + x]\n",
    "x_test = x_test[:y_t + x]\n",
    "y_test = y_test[:y_t + x]\n",
    "```\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 101502,
     "status": "ok",
     "timestamp": 1699537528404,
     "user": {
      "displayName": "Mehaboob Basha",
      "userId": "06121720411973053130"
     },
     "user_tz": -330
    },
    "id": "K80L-vVflLv4",
    "outputId": "6cf7584c-c85d-4144-c473-5343c516c6fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMAP test_label (427617,)\n",
      "SMAP train (135183, 25)\n",
      "SMAP test (427617, 25)\n"
     ]
    }
   ],
   "source": [
    "#python preprocess.py --dataset MSL\n",
    "!python preprocess.py --dataset SMAP\n",
    "# !python preprocess.py --dataset SMD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 258803,
     "status": "ok",
     "timestamp": 1699537937725,
     "user": {
      "displayName": "Mehaboob Basha",
      "userId": "06121720411973053130"
     },
     "user_tz": -330
    },
    "id": "zWJDZs_QF9gM",
    "outputId": "da297a6e-6531-414e-86e3-d3d6a69dbf31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-09 13:48:04.536097: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-09 13:48:04.536169: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-09 13:48:04.536210: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-09 13:48:04.549368: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-09 13:48:06.208897: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "{'arch': 1, 'dataset': 'SMAP', 'group': '1-1', 'lookback': 30, 'normalize': True, 'spec_res': False, 'kernel_size': 7, 'use_gatv2': False, 'feat_gat_embed_dim': None, 'time_gat_embed_dim': None, 'gru_n_layers': 5, 'gru_hid_dim': 150, 'fc_n_layers': 3, 'fc_hid_dim': 150, 'recon_n_layers': 1, 'recon_hid_dim': 150, 'alpha': 0.2, 'epochs': 1, 'val_split': 0.1, 'bs': 256, 'init_lr': 0.001, 'shuffle_dataset': True, 'dropout': 0.3, 'use_cuda': True, 'print_every': 1, 'log_tensorboard': True, 'scale_scores': False, 'use_mov_av': False, 'gamma': 1, 'level': None, 'q': None, 'dynamic_pot': False, 'comment': ''}\n",
      "1\n",
      "load data of: SMAP\n",
      "train:  0 None\n",
      "test:  0 None\n",
      "Data normalized\n",
      "Data normalized\n",
      "train set shape:  (135183, 25)\n",
      "test set shape:  (427617, 25)\n",
      "test set label shape:  (427617,)\n",
      "Will forecast and reconstruct input features: [0]\n",
      "train_size: 119808\n",
      "validation_size: 13312\n",
      "test_size: 427264\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "Init total train loss: 1.135431\n",
      "Init total val loss: 1.13286\n",
      "Training model for 1 epochs..\n",
      "[Epoch 1] forecast_loss = 0.14380, recon_loss = 0.11683, total_loss = 0.26063 ---- val_forecast_loss = 0.09477, val_recon_loss = 0.06694, val_total_loss = 0.16170 [24.7s]\n",
      "-- Training done in 24s.\n",
      "Test forecast loss: 0.08532\n",
      "Test reconstruction loss: 0.06183\n",
      "Test total loss: 0.14715\n",
      "False\n",
      "Predicting and calculating anomaly scores..\n",
      "100% 520/520 [00:25<00:00, 20.23it/s]\n",
      "Predicting and calculating anomaly scores..\n",
      "100% 1669/1669 [01:21<00:00, 20.53it/s]\n",
      "Running POT with q=0.005, level=0.9..\n",
      "100% 427264/427264 [00:00<00:00, 2125265.77it/s]\n",
      "Finding best f1-score by searching for threshold..\n",
      "Results using epsilon method:\n",
      " {'f1': 0.9023279189317025, 'precision': 0.9749941622672962, 'recall': 0.8397506214641379, 'TP': 45931, 'TN': 371390, 'FP': 1178, 'FN': 8765, 'threshold': 0.6856316775083542, 'latency': 172.4513347559117, 'reg_level': 0}\n",
      "Results using peak-over-threshold method:\n",
      " {'f1': 0.8793045295312651, 'precision': 0.9846125952399636, 'recall': 0.7943542487943626, 'TP': 43448, 'TN': 371889, 'FP': 679, 'FN': 11248, 'threshold': 0.7466014808617724, 'latency': 223.28775713939467}\n",
      "Results using best f1 score search:\n",
      " {'f1': 0.9539007291039819, 'precision': 0.911873561916596, 'recall': 0.9999999998171712, 'TP': 54696, 'TN': 367282, 'FP': 5286, 'FN': 0, 'threshold': 0.48760000000000037, 'latency': 160.13408935210543}\n",
      "Saving output to output/SMAP/09112023_134807/<train/test>_output.pkl\n",
      "-- Done.\n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataset SMAP --lookback 30 --epochs 1 --use_gatv2 False --arch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7cXMtxZYFmG"
   },
   "outputs": [],
   "source": [
    "datsets = ['SMD', 'SMAP']\n",
    "num_epochs = [1, 30, 50]\n",
    "for data in\n",
    "  for for epoc im\n",
    "\n",
    "    !nohup python preprocess.py --dataset data > log1.txt\n",
    "!nohup python train.py --dataset SMAP --lookback 30 --epochs 1 --use_gatv2 False --arch 1 > log2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFHh63PTlG5b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
